Earlier we mentioned that
machine learning is all about a computer learning the patterns
that distinguish things. Like for activity recognition, it was the pattern of walking, running and biking
that can be learned from various sensors on a device. To show how that works, let's take a look
at a set of numbers and see if you can determine
the pattern between them. Okay, here are the numbers. There's a formula
that maps X to Y. Can you spot it? Take a moment. Well, the answer is
Y equals 2X minus 1. So whenever you see a Y, it's twice the
corresponding X minus 1. If you figured it
out for yourself, well done, but how
did you do that? How would you think you
could figure this out? Maybe you can see that
the Y increases by 2 every time the
X increases by 1. So it probably
looks like Y equals 2X plus or minus something. Then when you saw X equals
0 and Y equals minus 1, so you thought hey that
the something is a minus 1, so the answer might be
Y equals 2X minus 1. You probably tried that
out with a couple of other values and
see that it fits. Congratulations, you've just done the basics of machine
learning in your head. So let's take a look
at it in code now. Okay, here's
our first line of code. This is written using
Python and TensorFlow and an API in TensorFlow
called keras. Keras makes it really easy
to define neural networks. A neural network
is basically a set of functions which
can learn patterns. Don't worry if there were
a lot of new concepts here. They will become clear quite quickly as you
work through them. The simplest possible
neural network is one that has
only one neuron in it, and that's what
this line of code does. In keras, you use the word dense to define a layer of
connected neurons. There's only one dense here. So there's only one layer and there's only one unit in it, so it's a single neuron. Successive layers are
defined in sequence, hence the word sequential. But as I've said,
there's only one. So you have a single neuron. You define the shape
of what's input to the neural network in the first and in this case the only layer, and you can see that
our input shape is super simple. It's
just one value. You've probably seen that
for machine learning, you need to know and
use a lot of math, calculus probability
and the like. It's really good to understand
that as you want to optimize your models
but the nice thing for now about TensorFlow
and keras is that a lot of that math is implemented
for you in functions. There are two function roles
that you should be aware of though and these are
loss functions and optimizers. This code defines them. I like to think
about it this way. The neural network has no idea of the relationship
between X and Y, so it makes a guess. Say it guesses
Y equals 10X minus 10. It will then use the data
that it knows about, that's the set of Xs and
Ys that we've already seen to measure how good or
how bad its guess was. The loss function measures
this and then gives the data to the optimizer which
figures out the next guess. So the optimizer thinks
about how good or how badly the guess was done using the data from
the loss function. Then the logic is that each guess should be better
than the one before. As the guesses get
better and better, an accuracy approaches
100 percent, the term convergence is used. In this case, the loss is
mean squared error and the optimizer is SGD which stands for stochastic
gradient descent. If you want to learn more about these particular functions, as well as the other
options that might be better in other scenarios, check out the TensorFlow
documentation. But for now we're just
going to use this. Our next step is to
represent the known data. These are the Xs and the Ys
that you saw earlier. The np.array is using
a Python library called numpy that makes
data representation particularly enlists much easier. So here you can see
we have one list for the Xs and another
one for the Ys. The training takes place
in the fit command. Here we're asking the model
to figure out how to fit the X values
to the Y values. The epochs equals
500 value means that it will go through the
training loop 500 times. This training loop is what
we described earlier. Make a guess, measure how good or how bad the guesses
with the loss function, then use the optimizer and the data to make another guess
and repeat this. When the model has
finished training, it will then give you back values using the predict method. So it hasn't previously seen 10, and what do you think it will return when you pass it a 10? Now you might think
it would return 19 because after all Y
equals 2X minus 1, and you think it should be 19. But when you try this in
the workbook yourself, you'll see that it will
return a value very close to 19 but not exactly 19. Now why do you think
that would be? Ultimately there are
two main reasons. The first is that you trained
it using very little data. There's only six points. Those six points are linear but there's no guarantee
that for every X, the relationship will
be Y equals 2X minus 1. There's a very high probability that Y equals 19 for X equals 10, but the neural network
isn't positive. So it will figure out
a realistic value for Y. That's the second main reason. When using neural networks, as they try to figure out
the answers for everything, they deal in probability. You'll see that a lot
and you'll have to adjust how you handle
answers to fit. Keep that in mind as you
work through the code. Okay, enough theory. Now let's get hands-on
and write the code that we just saw and
then we can run it.